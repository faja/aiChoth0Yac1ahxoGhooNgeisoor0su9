# vim: foldmethod=marker

# {{{ argo
snippet argo "argo" b
${0}---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ${1:`!v expand('%:p:h:t')`}
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io # provides CASCADE delete
spec:
  project: default
  source:
    repoURL: git@github.com:OnRecruitnl/onrecruit-infra.git
    targetRevision: master
    path: k8s/charts/${2:`!v expand('%:p:h:t')`}
    helm:
      releaseName: $2
      valueFiles:
        - ../../clusters/onrecruit-prd/apps/$1/values.yaml
        - ../../clusters/onrecruit-prd/apps/$1/values.appVersion.yaml
        - ../../clusters/onrecruit-prd/apps/$1/secrets.yaml
      version: v3
  destination:
    server: https://kubernetes.default.svc
    namespace: ${3:`!v expand('%:p:h:t')`}
  #syncPolicy: {}             # disables autosync
  syncPolicy:
    automated:                # triggers "sync" only if app is in green state
      prune: true             # remove not used resources (the ones that are not templated anymore)
      allowEmpty: false       # do not prune everything (safe check - do not prune if template is empty)
      selfHeal: false         # do not sync if we change something manually - only trigger sync for git changes
endsnippet
# }}}
# {{{ job
snippet job "Job" b
$0---
apiVersion: batch/v1
kind: Job
metadata:
  name: ${1:job_name}
spec:
  completions: ${2:1} # number of times the specified pod needs to run successfully, default = 1
  parallelism: ${3:1}
  backoffLimit: 2 
  activeDeadlineSeconds: 300
  ttlSecondsAfterFinished: 60
  template:
    metadata:
      name: $1
    spec:
      restartPolicy: OnFailure # only valid are: OnFailure, Never
      containers:
        - name: $1
          image: alpine:3
          command:
            - sh
            - -c
            - 'echo OK; sleep 10; echo OK2; sleep 10; exit 1'

# .spec.parallelism - how many Pods should run at once, default = 1
#                     setting this field to 0 effectively pasues the Job
#
# .spec.template.spec.restartPolicy - only valid options are OnFailure and Never
#  .spec.startingDeadlineSeconds - deadline in seconds for starting the Job if it misses its scheduled time
#
#  .spec.concurrencyPolicy - default `Allow`   - creates new Job even if the previous have not completed yet
#                                    `Forbid`  - does not create a new Job if something is already running
#                                    `Replace` - cancel curently running Job and start a new one
#
#  .spec.suspend - suspend all subsequent executions without affecting already started executions
#
#  spec.successfulJobsHistoryLimit (.spec.failedJobsHistoryLimit) - how many completed (failed) Jobs should be kept for auditing purposes
# 
# Never vs OnFailure
# a good practice is to set restartPolicy to 'OnFailure'
# the reason is: if the pod fails, the pod will be restarted not the JOB itself
# if restartPolicy is set to 'Never' - pod stays in failed state and new pod is created
# 
# jesli mamy to ustawione na Never - tworzone sa NOWE pody i w przypadku klapy joba
# nie znikaja one wogole, zostaja w stanie ERROR i mozemy sobie "podgladnac co sie stalo"
# Podow bedzie tyle ile mamy ustawiony backofflimit+1.
#
# w przypadku OnFailure - pod zostaje caly czas na miejscu, odpalany jest jedynie kontener
# w przypadku przekroczenia limitu backofflimit+1, pody sa usuwane, a job zostaje w stanie 0/1 copleted
# i ogolnie mamy takie info:
# ```
# Normal   SuccessfulCreate      2m31s  job-controller  Created pod: job-example-d6h94
# Normal   SuccessfulDelete      93s    job-controller  Deleted pod: job-example-d6h94
# Warning  BackoffLimitExceeded  93s    job-controller  Job has reached the specified backoff limit
# ```
#
#
# a jak to sie ma do cronjoba:
# zasada jest identyczna, cronjob tworzy joby, joby tworza pody z identyczna zasada jak wyrzej
endsnippet
# }}}
